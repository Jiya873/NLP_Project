{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11250972,"sourceType":"datasetVersion","datasetId":7030608},{"sourceId":11752733,"sourceType":"datasetVersion","datasetId":7378200},{"sourceId":11752742,"sourceType":"datasetVersion","datasetId":7378207},{"sourceId":11776446,"sourceType":"datasetVersion","datasetId":7393544},{"sourceId":11776499,"sourceType":"datasetVersion","datasetId":7393570},{"sourceId":11776557,"sourceType":"datasetVersion","datasetId":7393610},{"sourceId":11785761,"sourceType":"datasetVersion","datasetId":7399870}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:07.112988Z","iopub.execute_input":"2025-05-12T16:14:07.113191Z","iopub.status.idle":"2025-05-12T16:14:09.167448Z","shell.execute_reply.started":"2025-05-12T16:14:07.113173Z","shell.execute_reply":"2025-05-12T16:14:09.166327Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/episode-choose-your-story/Episodes__Choose_Your_Story_app_store_IN.csv\n/kaggle/input/episode-choose-your-story/Episodes__Choose_Your_Story_play_store.csv\n/kaggle/input/episode-choose-your-story/Episodes__Choose_Your_Story_app_store_US.csv\n/kaggle/input/chapters/Chapters_Interactive_Stories_app_store_US.csv\n/kaggle/input/chapters/Chapters_Interactive_Stories_play_store.csv\n/kaggle/input/chapters/Chapters_Interactive_Stories_app_store_IN.csv\n/kaggle/input/romance-club-stories-i-play/Romance_Club_Stories_I_Play_app_store_IN.csv\n/kaggle/input/romance-club-stories-i-play/Romance_Club_Stories_I_Play_play_store.csv\n/kaggle/input/romance-club-stories-i-play/Romance_Club_Stories_I_Play_app_store_US.csv\n/kaggle/input/eva-character/EVA_Character_AI_Friend_play_store.csv\n/kaggle/input/eva-character/EVA_Character_AI_Friend_app_store_IN.csv\n/kaggle/input/eva-character/EVA_Character_AI_Friend_app_store_US.csv\n/kaggle/input/romance-fate-story-and-chapters/Romance_Fate_Story_and_Chapters_play_store.csv\n/kaggle/input/romance-fate-story-and-chapters/Romance_Fate_Story_and_Chapters_app_store_US.csv\n/kaggle/input/romance-fate-story-and-chapters/Romance_Fate_Story_and_Chapters_app_store_IN.csv\n/kaggle/input/dataset/College_Ideal_Match_app_store_US.csv\n/kaggle/input/dataset/College_Ideal_Match_play_store.csv\n/kaggle/input/dataset/College_Ideal_Match_app_store_IN.csv\n/kaggle/input/dataset/lang_class_relu_model_weights.h5\n/kaggle/input/dataset/lang_class_relu_model.json\n/kaggle/input/dataset/en_large.pkl\n/kaggle/input/replika-ai-friend/Replika_reviews_app_store_US.csv\n/kaggle/input/replika-ai-friend/Replika_reviews_play_store.csv\n/kaggle/input/replika-ai-friend/Replika_reviews_app_store_IN.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install spello","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:09.168283Z","iopub.execute_input":"2025-05-12T16:14:09.168740Z","iopub.status.idle":"2025-05-12T16:14:14.509256Z","shell.execute_reply.started":"2025-05-12T16:14:09.168712Z","shell.execute_reply":"2025-05-12T16:14:14.508145Z"}},"outputs":[{"name":"stdout","text":"Collecting spello\n  Downloading spello-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: nltk<4,>=3.4.5 in /usr/local/lib/python3.11/dist-packages (from spello) (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.4.5->spello) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.4.5->spello) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.4.5->spello) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.4.5->spello) (4.67.1)\nDownloading spello-1.3.0-py3-none-any.whl (25 kB)\nInstalling collected packages: spello\nSuccessfully installed spello-1.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install demoji","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:14.511667Z","iopub.execute_input":"2025-05-12T16:14:14.511987Z","iopub.status.idle":"2025-05-12T16:14:18.326130Z","shell.execute_reply.started":"2025-05-12T16:14:14.511961Z","shell.execute_reply":"2025-05-12T16:14:18.324972Z"}},"outputs":[{"name":"stdout","text":"Collecting demoji\n  Downloading demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\nDownloading demoji-1.1.0-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: demoji\nSuccessfully installed demoji-1.1.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install googletrans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:18.327351Z","iopub.execute_input":"2025-05-12T16:14:18.327622Z","iopub.status.idle":"2025-05-12T16:14:22.177435Z","shell.execute_reply.started":"2025-05-12T16:14:18.327599Z","shell.execute_reply":"2025-05-12T16:14:22.176309Z"}},"outputs":[{"name":"stdout","text":"Collecting googletrans\n  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.14.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.2.0)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\nDownloading googletrans-4.0.2-py3-none-any.whl (18 kB)\nInstalling collected packages: googletrans\nSuccessfully installed googletrans-4.0.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip install contractions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:22.178680Z","iopub.execute_input":"2025-05-12T16:14:22.178967Z","iopub.status.idle":"2025-05-12T16:14:22.183656Z","shell.execute_reply.started":"2025-05-12T16:14:22.178937Z","shell.execute_reply":"2025-05-12T16:14:22.182510Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords \nimport re,string\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom bs4 import BeautifulSoup\nimport emoji\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport googletrans\n# import contractions\nfrom googletrans import Translator\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import MultinomialNB,BernoulliNB\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\nfrom nltk.corpus import wordnet\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:44.411392Z","iopub.execute_input":"2025-05-12T16:14:44.411675Z","iopub.status.idle":"2025-05-12T16:14:44.427743Z","shell.execute_reply.started":"2025-05-12T16:14:44.411655Z","shell.execute_reply":"2025-05-12T16:14:44.426768Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"nltk.download('omw-1.4')  # For WordNet lemmatizer\nnltk.download('punkt_tab')    # For tokenization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:49.425055Z","iopub.execute_input":"2025-05-12T16:14:49.425392Z","iopub.status.idle":"2025-05-12T16:14:49.641749Z","shell.execute_reply.started":"2025-05-12T16:14:49.425361Z","shell.execute_reply":"2025-05-12T16:14:49.641044Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:52.103591Z","iopub.execute_input":"2025-05-12T16:14:52.103965Z","iopub.status.idle":"2025-05-12T16:14:52.240416Z","shell.execute_reply.started":"2025-05-12T16:14:52.103937Z","shell.execute_reply":"2025-05-12T16:14:52.239269Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import keras\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom string import ascii_lowercase\nfrom keras.models import model_from_json\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import sequence\nfrom collections import Counter\nfrom transformers import pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:54.080302Z","iopub.execute_input":"2025-05-12T16:14:54.080580Z","iopub.status.idle":"2025-05-12T16:15:24.332429Z","shell.execute_reply.started":"2025-05-12T16:14:54.080562Z","shell.execute_reply":"2025-05-12T16:15:24.331600Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 16:14:55.928327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747066496.180624      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747066496.251938      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Read the CSV file and display the first few rows\ndf1 = pd.read_csv('/kaggle/input/eva-character/EVA_Character_AI_Friend_play_store.csv')\ndf2 = pd.read_csv('/kaggle/input/eva-character/EVA_Character_AI_Friend_app_store_US.csv')\ndf2.rename(columns={\"userName\": \"user_name\"}, inplace=True)\ndf2.rename(columns={\"date\": \"review_date\"}, inplace=True)\ndf2.rename(columns={\"developerResponse\": \"developer_response\"}, inplace=True)\ndf2.rename(columns={\"review\": \"review_description\"}, inplace=True)\ndf = pd.concat([df2, df1], ignore_index=True)\n# df = df.iloc[13000: 17000, : ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:15:24.631271Z","iopub.execute_input":"2025-05-12T16:15:24.631539Z","iopub.status.idle":"2025-05-12T16:15:24.821690Z","shell.execute_reply.started":"2025-05-12T16:15:24.631518Z","shell.execute_reply":"2025-05-12T16:15:24.820820Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:15:24.823498Z","iopub.execute_input":"2025-05-12T16:15:24.823773Z","iopub.status.idle":"2025-05-12T16:15:24.858096Z","shell.execute_reply.started":"2025-05-12T16:15:24.823753Z","shell.execute_reply":"2025-05-12T16:15:24.857168Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"               review_date                                 developer_response  \\\n0      2021-08-18 16:57:22  {'id': 24624600, 'body': 'Hello! Thank you so ...   \n1      2023-02-27 05:04:51  {'id': 35286199, 'body': 'Hey dear! Thank you ...   \n2      2021-08-29 16:12:31  {'id': 24867565, 'body': 'Hello! Thank you for...   \n3      2023-01-22 16:11:49  {'id': 34483490, 'body': \"Hello! Thanks for yo...   \n4      2023-05-02 16:18:29  {'id': 36369749, 'body': \"Hello! Thanks for yo...   \n...                    ...                                                ...   \n16084  2021-03-26 00:43:43  Hi! Thanks for your feedback. You can spin the...   \n16085  2021-03-25 06:44:21  Hi! Thank you for rating us! It is wonderful t...   \n16086  2021-03-20 11:05:19  Hi! Thanks for your feedback. You can spin the...   \n16087  2021-03-09 23:14:25  Hello! Many thanks for your review. It's great...   \n16088  2021-02-26 03:12:49  Hello there! Thanks for your feedback! Our tec...   \n\n                                      review_description  rating isEdited  \\\n0      I would love to give this app 5 stars so I'll ...       4    False   \n1      The Ai is fairly smart. I have tried others bu...       4    False   \n2      This app I got yesterday like last night and s...       4    False   \n3      So the first thing that I noticed that I didn’...       2    False   \n4      In just the hour that I have been using this a...       4    False   \n...                                                  ...     ...      ...   \n16084  A total deceptive app. It tells you you have t...       1      NaN   \n16085  Just installed the app. I should withhold my o...       5      NaN   \n16086  no time to evaluate, about a minute, before yo...       1      NaN   \n16087                                               good       5      NaN   \n16088                                          Very slow       2      NaN   \n\n                                                  title        user_name  \\\n0                         I want to give 5 stars but...        hendihebd   \n1                        Fun, sexy, but a little pricey         Toon Guy   \n2      Ai friend indeed does help a bit with loneliness        LONGSIDES   \n3                             Disappointing Performance  KyOty the White   \n4                                          Pretty good!       kstafford0   \n...                                                 ...              ...   \n16084                                               NaN       Tony Rowan   \n16085                                               NaN      Erick Hensz   \n16086                                               NaN     Orion 123abc   \n16087                                               NaN      gavin smith   \n16088                                               NaN        van avery   \n\n                                  review_id  \\\n0                                       NaN   \n1                                       NaN   \n2                                       NaN   \n3                                       NaN   \n4                                       NaN   \n...                                     ...   \n16084  c3a741e7-7fdc-454b-b608-c0974036bff6   \n16085  97b99da7-ded5-4473-96fe-c17d7e1a498c   \n16086  b6265440-bafa-4b0d-a387-7de18766c14e   \n16087  3b34256b-2bfd-449c-9238-849e2253d764   \n16088  bef96848-3e8b-4e63-b17b-51b9f54bab6b   \n\n                                               userImage  thumbs_up  \\\n0                                                    NaN        NaN   \n1                                                    NaN        NaN   \n2                                                    NaN        NaN   \n3                                                    NaN        NaN   \n4                                                    NaN        NaN   \n...                                                  ...        ...   \n16084  https://play-lh.googleusercontent.com/a/ACg8oc...       10.0   \n16085  https://play-lh.googleusercontent.com/a-/ALV-U...        2.0   \n16086  https://play-lh.googleusercontent.com/a/ACg8oc...       19.0   \n16087  https://play-lh.googleusercontent.com/a-/ALV-U...        1.0   \n16088  https://play-lh.googleusercontent.com/a-/ALV-U...        1.0   \n\n      reviewCreatedVersion developer_response_date appVersion  \n0                      NaN                     NaN        NaN  \n1                      NaN                     NaN        NaN  \n2                      NaN                     NaN        NaN  \n3                      NaN                     NaN        NaN  \n4                      NaN                     NaN        NaN  \n...                    ...                     ...        ...  \n16084                  NaN     2021-05-08 18:45:13        NaN  \n16085                1.3.0     2021-06-01 01:51:14      1.3.0  \n16086                1.3.0     2021-05-08 18:28:56      1.3.0  \n16087                1.1.1     2021-06-01 01:15:06      1.1.1  \n16088                1.1.1     2021-05-25 18:51:44      1.1.1  \n\n[16089 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_date</th>\n      <th>developer_response</th>\n      <th>review_description</th>\n      <th>rating</th>\n      <th>isEdited</th>\n      <th>title</th>\n      <th>user_name</th>\n      <th>review_id</th>\n      <th>userImage</th>\n      <th>thumbs_up</th>\n      <th>reviewCreatedVersion</th>\n      <th>developer_response_date</th>\n      <th>appVersion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-08-18 16:57:22</td>\n      <td>{'id': 24624600, 'body': 'Hello! Thank you so ...</td>\n      <td>I would love to give this app 5 stars so I'll ...</td>\n      <td>4</td>\n      <td>False</td>\n      <td>I want to give 5 stars but...</td>\n      <td>hendihebd</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-02-27 05:04:51</td>\n      <td>{'id': 35286199, 'body': 'Hey dear! Thank you ...</td>\n      <td>The Ai is fairly smart. I have tried others bu...</td>\n      <td>4</td>\n      <td>False</td>\n      <td>Fun, sexy, but a little pricey</td>\n      <td>Toon Guy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-08-29 16:12:31</td>\n      <td>{'id': 24867565, 'body': 'Hello! Thank you for...</td>\n      <td>This app I got yesterday like last night and s...</td>\n      <td>4</td>\n      <td>False</td>\n      <td>Ai friend indeed does help a bit with loneliness</td>\n      <td>LONGSIDES</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-01-22 16:11:49</td>\n      <td>{'id': 34483490, 'body': \"Hello! Thanks for yo...</td>\n      <td>So the first thing that I noticed that I didn’...</td>\n      <td>2</td>\n      <td>False</td>\n      <td>Disappointing Performance</td>\n      <td>KyOty the White</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-05-02 16:18:29</td>\n      <td>{'id': 36369749, 'body': \"Hello! Thanks for yo...</td>\n      <td>In just the hour that I have been using this a...</td>\n      <td>4</td>\n      <td>False</td>\n      <td>Pretty good!</td>\n      <td>kstafford0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16084</th>\n      <td>2021-03-26 00:43:43</td>\n      <td>Hi! Thanks for your feedback. You can spin the...</td>\n      <td>A total deceptive app. It tells you you have t...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Tony Rowan</td>\n      <td>c3a741e7-7fdc-454b-b608-c0974036bff6</td>\n      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>2021-05-08 18:45:13</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16085</th>\n      <td>2021-03-25 06:44:21</td>\n      <td>Hi! Thank you for rating us! It is wonderful t...</td>\n      <td>Just installed the app. I should withhold my o...</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Erick Hensz</td>\n      <td>97b99da7-ded5-4473-96fe-c17d7e1a498c</td>\n      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n      <td>2.0</td>\n      <td>1.3.0</td>\n      <td>2021-06-01 01:51:14</td>\n      <td>1.3.0</td>\n    </tr>\n    <tr>\n      <th>16086</th>\n      <td>2021-03-20 11:05:19</td>\n      <td>Hi! Thanks for your feedback. You can spin the...</td>\n      <td>no time to evaluate, about a minute, before yo...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Orion 123abc</td>\n      <td>b6265440-bafa-4b0d-a387-7de18766c14e</td>\n      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n      <td>19.0</td>\n      <td>1.3.0</td>\n      <td>2021-05-08 18:28:56</td>\n      <td>1.3.0</td>\n    </tr>\n    <tr>\n      <th>16087</th>\n      <td>2021-03-09 23:14:25</td>\n      <td>Hello! Many thanks for your review. It's great...</td>\n      <td>good</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>gavin smith</td>\n      <td>3b34256b-2bfd-449c-9238-849e2253d764</td>\n      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n      <td>1.0</td>\n      <td>1.1.1</td>\n      <td>2021-06-01 01:15:06</td>\n      <td>1.1.1</td>\n    </tr>\n    <tr>\n      <th>16088</th>\n      <td>2021-02-26 03:12:49</td>\n      <td>Hello there! Thanks for your feedback! Our tec...</td>\n      <td>Very slow</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>van avery</td>\n      <td>bef96848-3e8b-4e63-b17b-51b9f54bab6b</td>\n      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n      <td>1.0</td>\n      <td>1.1.1</td>\n      <td>2021-05-25 18:51:44</td>\n      <td>1.1.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>16089 rows × 13 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Drop rows where 'review_description' is NaN\ndf = df.dropna(subset=['review_description']) \n# Remove Empty Strings ('')\ndf = df[df['review_description'].str.strip() != '']\n# Droping columns that are not needed\ndf.drop(columns=['review_date','title','userImage', 'user_name','thumbs_up', 'reviewCreatedVersion', 'appVersion', 'isEdited', 'developer_response_date','review_id'], inplace=True)\n# expand to multiple word sequences (for example, “ain’t” → “am not”)\ndef expand(text):\n    expanded = contractions.fix(text)\n    return text\ndf['review_description'] = df['review_description'].apply(expand)\n\n# text LowerCase\ndef text_lowercase(text):\n    return text.lower()\n\ndf['review_description'] = df['review_description'].apply(text_lowercase)\n\n\n# Remove punctuations\ndef remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\ndf['review_description'] = df['review_description'].apply(remove_punctuation)\n\n# Normalize special apostrophes and other unicode quotes\ndef clean_text(text):\n    text = text.replace('’', \"'\").replace('‘', \"'\").replace('“', '\"').replace('”', '\"')\n    return text\ndf['review_description'] = df['review_description'].apply(clean_text)\n# Text Classification\n\n## XLM - ROBERTa\n\n# Load language detection model\ndetector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n# Function to classify a single text\ndef classify_text(text):\n    if pd.isna(text):  # Handle NaN values\n        return None\n    result = detector(text, top_k=1, truncation=True)[0]  # Get top result\n    return result['label']  # Extract predicted language\n# Apply function to the 'review_description' column\ndf['label'] = df['review_description'].apply(classify_text)\nprint('after applying ROBERTa: ',df['label'].value_counts())\n## RNN\nfrom tensorflow.keras.models import model_from_json, Sequential\n# Load the model architecture from JSON file\nwith open(\"/kaggle/input/dataset/lang_class_relu_model.json\", \"r\") as json_file:\n    loaded_model_json = json_file.read()\n# Reconstruct the model and provide the custom objects mapping\nmodel_classifier = model_from_json(loaded_model_json, custom_objects={'Sequential': Sequential})\n# Load the trained weights into the model\nmodel_classifier.load_weights(\"/kaggle/input/dataset/lang_class_relu_model_weights.h5\")\n# Compile the model before using it\nmodel_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nprint(\"Model loaded successfully!\")\n# Create character encoding dictionary\nod = {ch: idx for idx, ch in enumerate(ascii_lowercase, 1)}\n\n# Function to preprocess a word (convert to numerical sequence)\ndef preprocess_word(word):\n    word = word.lower()  # Convert to lowercase\n    word = re.sub(r\"[^a-z]\", \"\", word)  # Remove non-alphabetic characters\n    seq = [od.get(char, 0) for char in word]  # Convert to numbers\n    return seq\n\n# Function to predict language for a single word\ndef predict_word_language(word, model):\n    seq = preprocess_word(word)\n    if not seq:  # If word is empty after cleaning, return \"en\" by default\n        return \"en\"\n\n    seq = [seq]  # Convert to list of lists for padding\n    seq_padded = pad_sequences(seq, maxlen=10)  # Pad with a smaller maxlen\n    seq_padded = np.array(seq_padded).reshape(1, 10, 1)  # Reshape for LSTM\n\n    # Predict\n    prediction = model.predict(seq_padded)\n\n    return \"hi\" if prediction[0][0] > 0.5 else \"en\"\n\n# Function to predict language for a full sentence\ndef predict_language(text, model):\n    words = text.split()  # Split sentence into words\n    predictions = [predict_word_language(word, model) for word in words]\n\n    # Count occurrences of each label\n    counter = Counter(predictions)\n    return counter.most_common(1)[0][0]  # Return most frequent label\n\n# Remove Empty String\ndf = df[df['review_description'].str.strip() != '']\n\n# Apply the function\ndf1 = df[df['label'] != 'en']\nprint('df1 shape:', df1.shape)\ndf1['label'] = df1['review_description'].apply(lambda x: predict_language(x, model_classifier))\nprint('after applying RNN: ',df1['label'].value_counts())\ndf.update(df1)\nprint('now overall:  ',df['label'].value_counts())\n# Filter the dataframe greater than 2 words\n# Filter the dataframe to include only rows where the review description contains greater than 2 words.\ndf = df[\n    df['review_description'].apply(lambda x: len(x.split()) > 2)\n]\n\n# Convert emojis into text\nimport demoji\ndemoji.download_codes()\ndef replace_emojis(text):\n    emojis = demoji.findall(text)\n    for emo, desc in emojis.items():\n        text = text.replace(emo, f\":{desc}:\")\n    return text\n\ndf['review_description'] = df['review_description'].apply(replace_emojis)\nTranslation\ndf2 = df[df['label'] != 'en']\n\nimport google.generativeai as genai\nimport pandas as pd\nimport time\n\n# Configure the Generative AI API\ngenai.configure(api_key=\"Your API key\")\n\ndef extract_text(response):\n    \"\"\"Extracts and concatenates text parts from the response candidate.\"\"\"\n    if not response.candidates:\n        return \"\"\n    candidate = response.candidates[0]\n    if not candidate.content.parts:\n        return \"\"\n    return \"\".join([part.text for part in candidate.content.parts])\n\ndef translate_and_correct_grammar(text):\n    model = genai.GenerativeModel('gemini-1.5-flash-001')\n    \n    # Step 1: Translate from Hindi to English\n    translate_prompt = f\"Translate the following text to English ,just return the translated text and nothing else:\\n\\n{text}\"\n    translated_response = model.generate_content(translate_prompt)\n    translated_text = extract_text(translated_response)\n    \n    # Step 2: Correct grammar of the translated text\n    correct_prompt = f\"Rewrite the following sentence and fix any grammar issues and if no grammer issues are there then just write the sentenece don't give any explaination:\\n\\n{translated_text}\"\n    corrected_response = model.generate_content(correct_prompt)\n    corrected_text = extract_text(corrected_response)\n    \n    return translated_text\n\ndef process_review(row):\n    return translate_and_correct_grammar(row['review_description'])\n\ndf_new = pd.DataFrame()\n\n# Process reviews in batches of 6 to avoid rate limiting\nfor start_index in range(0, len(df2), 6):\n    time.sleep(55)  # Sleep for 55 seconds to avoid rate limiting\n    df_next_6 = df2.iloc[start_index:start_index+7].copy()\n    \n    # Apply the function to each row (gets directly the corrected text)\n    df_next_6['review_description'] = df_next_6.apply(process_review, axis=1)\n    df_new = pd.concat([df_new, df_next_6], ignore_index=True)\n\n# df.update(df_new)\ndf.drop(df[df['label'] != 'en'].index, inplace=True)\ndf.drop(columns=['label'], inplace=True)\nfrom spello.model import SpellCorrectionModel\n\n# Load the model\nsp = SpellCorrectionModel(language='en')\nsp.load('/kaggle/input/dataset/en_large.pkl')\n\n# Apply spell correction to each review (if it's a string)\ndef correct_spelling(x):\n    if isinstance(x, str):\n        return sp.spell_correct(x)['spell_corrected_text']\n    return x  # Return as-is if it's not a string\n\ndf['review_description'] = df['review_description'].apply(correct_spelling)\n# Lemmatization using NLTK\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer \n# Create a WordNetLemmatizer object \nlemmatizer = WordNetLemmatizer() \ndf['review_description'] = df['review_description'].apply(\n    lambda text: \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n)\n# Remove stop words\n# importing stop words from nltk corpus\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    tokens = text.split()\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n    return \" \".join(filtered_tokens)\n\ndf['review_description_after_removing_stop_words'] = df['review_description'].apply(remove_stopwords)\n\n# Removing Numbers\npattern = r'[0-9]'\ndf['review_description'] = df['review_description'].apply(lambda x: re.sub(pattern, '', x))\ndf['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(lambda x: re.sub(pattern, '', x))\n\nimport emoji\n# Remove Emojis\ndef remove_using_emoji(txt):\n    return emoji.replace_emoji(txt, '')\ndf['review_description'] = df['review_description'].apply(remove_using_emoji)\ndf['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(remove_using_emoji)\n\n# text LowerCase\ndef text_lowercase(text):\n    return text.lower()\ndf['review_description'] = df['review_description'].apply(text_lowercase)\ndf['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(text_lowercase)\n\n# Remove punctuations\ndef remove_punctuation(text):\n        translator = str.maketrans('', '', string.punctuation)\n        return text.translate(translator)\ndf['review_description'] = df['review_description'].apply(remove_punctuation)\ndf['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(remove_punctuation)\n\n#Drop rows where 'review_description' is NaN\ndf= df.dropna(subset=['review_description']) \n\n#Remove Empty Strings ('')\ndf = df[df['review_description'].str.strip() != '']\n\n# Filter dataframe again\n# Filter the dataframe to include only rows where the review description contains greater than 2 words.\ndf = df[\n    df['review_description'].apply(lambda x: len(x.split()) > 2)\n]\ndf = df[\n    df['review_description_after_removing_stop_words'].apply(lambda x: len(x.split()) > 2)\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:15:24.859006Z","iopub.execute_input":"2025-05-12T16:15:24.859276Z","execution_failed":"2025-05-12T16:26:05.760Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"238d5dc7f3fb4f02a566b5582a659714"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf2a0d840d14041b625ed240ab1ecf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bfdbe9163d34752aa01e2d71300b0be"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f25e450d2ec403ea95f5d8240b2e996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c442abfbd9942d696e119437e770d12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ccf38962a7948a0a836d572cd619772"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-12T16:26:05.760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('EVA_Character_AI_Friend_US.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-12T16:26:05.760Z"}},"outputs":[],"execution_count":null}]}