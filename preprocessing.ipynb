{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-kerasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jiyaa\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (78.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jiyaa\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jiyaa\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Installing collected packages: tensorflow, tf-keras\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "Successfully installed tensorflow-2.19.0 tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Using cached demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Using cached demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\jiyaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords \n",
    "import re,string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from string import ascii_lowercase\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file and display the first few rows\n",
    "df1 = pd.read_csv('Family_Hotel_love_&_match_3_play_store.csv')\n",
    "df2 = pd.read_csv('Family_Hotel_love_&_match_3_app_store_US.csv')\n",
    "df2.rename(columns={\"userName\": \"user_name\"}, inplace=True)\n",
    "df2.rename(columns={\"date\": \"review_date\"}, inplace=True)\n",
    "df2.rename(columns={\"developerResponse\": \"developer_response\"}, inplace=True)\n",
    "df2.rename(columns={\"review\": \"review_description\"}, inplace=True)\n",
    "df = pd.concat([df2, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:151: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:151: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\jiyaa\\AppData\\Local\\Temp\\ipykernel_25156\\686335609.py:151: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  sp.load('en_large.pkl\\en_large.pkl')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where 'review_description' is NaN\n",
    "df = df.dropna(subset=['review_description']) \n",
    "# Remove Empty Strings ('')\n",
    "df = df[df['review_description'].str.strip() != '']\n",
    "# Droping columns that are not needed\n",
    "df.drop(columns=['userImage', 'user_name', 'reviewCreatedVersion', 'appVersion', 'isEdited', 'developer_response_date'], inplace=True)\n",
    "# text LowerCase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['review_description'] = df['review_description'].apply(text_lowercase)\n",
    "# Remove punctuations\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "df['review_description'] = df['review_description'].apply(remove_punctuation)\n",
    "# Text Classification\n",
    "\n",
    "## XLM - ROBERTa\n",
    "\n",
    "# Load language detection model\n",
    "detector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "# Function to classify a single text\n",
    "def classify_text(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return None\n",
    "    result = detector(text, top_k=1, truncation=True)[0]  # Get top result\n",
    "    return result['label']  # Extract predicted language\n",
    "# Apply function to the 'review_description' column\n",
    "df['label'] = df['review_description'].apply(classify_text)\n",
    "print('after applying ROBERTa: ',df['label'].value_counts())\n",
    "## RNN\n",
    "from tensorflow.keras.models import model_from_json, Sequential\n",
    "# Load the model architecture from JSON file\n",
    "with open(\"lang_class_relu_model.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "# Reconstruct the model and provide the custom objects mapping\n",
    "model_classifier = model_from_json(loaded_model_json, custom_objects={'Sequential': Sequential})\n",
    "# Load the trained weights into the model\n",
    "model_classifier.load_weights(\"lang_class_relu_model_weights.h5\")\n",
    "# Compile the model before using it\n",
    "model_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Model loaded successfully!\")\n",
    "# Create character encoding dictionary\n",
    "od = {ch: idx for idx, ch in enumerate(ascii_lowercase, 1)}\n",
    "# Function to preprocess a word (convert to numerical sequence)\n",
    "def preprocess_word(word):\n",
    "    word = word.lower()  # Convert to lowercase\n",
    "    word = re.sub(r\"[^a-z]\", \"\", word)  # Remove non-alphabetic characters\n",
    "    seq = [od.get(char, 0) for char in word]  # Convert to numbers\n",
    "    return seq\n",
    "\n",
    "# Function to predict language for a single word\n",
    "def predict_word_language(word, model):\n",
    "    seq = preprocess_word(word)\n",
    "    if not seq:  # If word is empty after cleaning, return \"en\" by default\n",
    "        return \"en\"\n",
    "\n",
    "    seq = [seq]  # Convert to list of lists for padding\n",
    "    seq_padded = pad_sequences(seq, maxlen=10)  # Pad with a smaller maxlen\n",
    "    seq_padded = np.array(seq_padded).reshape(1, 10, 1)  # Reshape for LSTM\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(seq_padded)\n",
    "\n",
    "    return \"hi\" if prediction[0][0] > 0.5 else \"en\"\n",
    "\n",
    "# Function to predict language for a full sentence\n",
    "def predict_language(text, model):\n",
    "    words = text.split()  # Split sentence into words\n",
    "    predictions = [predict_word_language(word, model) for word in words]\n",
    "\n",
    "    # Count occurrences of each label\n",
    "    counter = Counter(predictions)\n",
    "    return counter.most_common(1)[0][0]  # Return most frequent label\n",
    "\n",
    "df1 = df[df['label'] != 'en']\n",
    "print('df1 shape:', df1.shape)\n",
    "df1['label'] = df1['review_description'].apply(lambda x: predict_language(x, model_classifier))\n",
    "print('after applying RNN: ',df1['label'].value_counts())\n",
    "df.update(df1)\n",
    "print('now overall:  ',df['label'].value_counts())\n",
    "# Filter the dataframe greater than 2 words\n",
    "# Filter the dataframe to include only rows where the review description contains greater than 2 words.\n",
    "df = df[\n",
    "    df['review_description'].apply(lambda x: len(x.split()) > 2)\n",
    "]\n",
    "# Convert emojis into text\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "def replace_emojis(text):\n",
    "    emojis = demoji.findall(text)\n",
    "    for emo, desc in emojis.items():\n",
    "        text = text.replace(emo, f\":{desc}:\")\n",
    "    return text\n",
    "\n",
    "df['review_description'] = df['review_description'].apply(replace_emojis)\n",
    "# Translation\n",
    "df2 = df[df['label'] != 'en']\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure the Generative AI API\n",
    "genai.configure(api_key=\"AIzaSyDL9ENAovM3QHePqPuKN6jHRryTgMRIH3g\")\n",
    "\n",
    "def extract_text(response):\n",
    "    \"\"\"Extracts and concatenates text parts from the response candidate.\"\"\"\n",
    "    if not response.candidates:\n",
    "        return \"\"\n",
    "    candidate = response.candidates[0]\n",
    "    if not candidate.content.parts:\n",
    "        return \"\"\n",
    "    return \"\".join([part.text for part in candidate.content.parts])\n",
    "\n",
    "def translate_and_correct_grammar(text):\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-001')\n",
    "    \n",
    "    # Step 1: Translate from Hindi to English\n",
    "    translate_prompt = f\"Translate the following text to English ,just return the translated text and nothing else:\\n\\n{text}\"\n",
    "    translated_response = model.generate_content(translate_prompt)\n",
    "    translated_text = extract_text(translated_response)\n",
    "    \n",
    "    # Step 2: Correct grammar of the translated text\n",
    "    correct_prompt = f\"Rewrite the following sentence and fix any grammar issues and if no grammer issues are there then just write the sentenece don't give any explaination:\\n\\n{translated_text}\"\n",
    "    corrected_response = model.generate_content(correct_prompt)\n",
    "    corrected_text = extract_text(corrected_response)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "def process_review(row):\n",
    "    return translate_and_correct_grammar(row['review_description'])\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "# Process reviews in batches of 7 to avoid rate limiting\n",
    "for start_index in range(0, len(df2), 7):\n",
    "    time.sleep(50)  # Sleep for 50 seconds to avoid rate limiting\n",
    "    df_next_7 = df2.iloc[start_index:start_index+7].copy()\n",
    "    \n",
    "    # Apply the function to each row (gets directly the corrected text)\n",
    "    df_next_7['review_description'] = df_next_7.apply(process_review, axis=1)\n",
    "    df_new = pd.concat([df_new, df_next_7], ignore_index=True)\n",
    "\n",
    "df.update(df_new)\n",
    "df.drop(columns=['label'], inplace=True)\n",
    "# Spelling correction\n",
    "from spello.model import SpellCorrectionModel\n",
    "sp = SpellCorrectionModel(language='en')\n",
    "sp.load('en_large.pkl\\en_large.pkl')\n",
    "df['review_description'] = df['review_description'].apply(lambda x: x.get('spell_corrected_text') if isinstance(x, dict) and 'spell_corrected_text' in x else x)\n",
    "# Lemmatization using NLTK\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# Create a WordNetLemmatizer object \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "df['review_description'] = df['review_description'].apply(\n",
    "    lambda text: \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
    ")\n",
    "# Remove stop words\n",
    "# importing stop words from nltk corpus\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df['review_description_after_removing_stop_words'] = df['review_description'].apply(remove_stopwords)\n",
    "\n",
    "# Removing Numbers\n",
    "pattern = r'[0-9]'\n",
    "df['review_description'] = df['review_description'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "import emoji\n",
    "# Remove Emojis\n",
    "def remove_using_emoji(txt):\n",
    "    return emoji.replace_emoji(txt, '')\n",
    "df['review_description'] = df['review_description'].apply(remove_using_emoji)\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(remove_using_emoji)\n",
    "\n",
    "# text LowerCase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "df['review_description'] = df['review_description'].apply(text_lowercase)\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(text_lowercase)\n",
    "\n",
    "# Remove punctuations\n",
    "def remove_punctuation(text):\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        return text.translate(translator)\n",
    "df['review_description'] = df['review_description'].apply(remove_punctuation)\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(remove_punctuation)\n",
    "\n",
    "#Drop rows where 'review_description' is NaN\n",
    "df= df.dropna(subset=['review_description']) \n",
    "\n",
    "#Remove Empty Strings ('')\n",
    "df = df[df['review_description'].str.strip() != '']\n",
    "\n",
    "# Filter dataframe again\n",
    "# Filter the dataframe to include only rows where the review description contains greater than 2 words.\n",
    "df = df[\n",
    "    df['review_description'].apply(lambda x: len(x.split()) > 2)\n",
    "]\n",
    "df = df[\n",
    "    df['review_description_after_removing_stop_words'].apply(lambda x: len(x.split()) > 2)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jiyaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jiyaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jiyaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 shape: (6308, 8)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf1 shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, df1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 59\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_classifier\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter applying RNN: \u001b[39m\u001b[38;5;124m'\u001b[39m, df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[0;32m     61\u001b[0m df\u001b[38;5;241m.\u001b[39mupdate(df1)\n",
      "File \u001b[1;32mc:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiyaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[64], line 59\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     57\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf1 shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, df1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 59\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpredict_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_classifier\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter applying RNN: \u001b[39m\u001b[38;5;124m'\u001b[39m, df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[0;32m     61\u001b[0m df\u001b[38;5;241m.\u001b[39mupdate(df1)\n",
      "Cell \u001b[1;32mIn[64], line 54\u001b[0m, in \u001b[0;36mpredict_language\u001b[1;34m(text, model)\u001b[0m\n\u001b[0;32m     52\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [predict_word_language(word, model) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     53\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter(predictions)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_common\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import demoji\n",
    "import emoji\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from spello.model import SpellCorrectionModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Make sure required NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# A simple preprocessor function for a single word\n",
    "def preprocess_word(word):\n",
    "    word = word.lower().strip()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return word.translate(translator)\n",
    "\n",
    "# Fix: predict the language of a single word using model_classifier\n",
    "def predict_word_language(word, model):\n",
    "    cleaned_word = preprocess_word(word)\n",
    "    if not cleaned_word:\n",
    "        return \"en\"\n",
    "    \n",
    "    # Convert the cleaned word to a sequence of integers using the od mapping.\n",
    "    # If a character is not found in the mapping, default to 0.\n",
    "    tokenized = [od.get(ch, 0) for ch in cleaned_word]\n",
    "    # Wrap the tokenized sequence in a list and pad it.\n",
    "    tokenized = [tokenized]\n",
    "    seq_padded = pad_sequences(tokenized, maxlen=30)  # Use sequence length expected by the model\n",
    "    seq_padded = np.expand_dims(seq_padded, -1)  # Add the feature dimension (batch_size, 30, 1)\n",
    "    \n",
    "    prediction = model.predict(seq_padded)\n",
    "    if prediction.shape[0] < 1 or prediction.shape[1] < 1:\n",
    "        return \"en\"\n",
    "    \n",
    "    # Return \"hi\" if predicted probability > 0.5 else \"en\"\n",
    "    return \"hi\" if prediction[0][0] > 0.5 else \"en\"\n",
    "\n",
    "# Predict language for full sentence by aggregating word predictions\n",
    "def predict_language(text, model):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return \"en\"\n",
    "    predictions = [predict_word_language(word, model) for word in words]\n",
    "    counter = Counter(predictions)\n",
    "    if counter:\n",
    "        return counter.most_common(1)[0][0]\n",
    "    return \"en\"\n",
    "\n",
    "# Update non-English rows in df1 using predict_language\n",
    "df1 = df[df['label'] != 'en'].copy()\n",
    "print('df1 shape:', df1.shape)\n",
    "df1['label'] = df1['review_description'].apply(lambda x: predict_language(x, model_classifier))\n",
    "print('after applying RNN: ', df1['label'].value_counts())\n",
    "df.update(df1)\n",
    "print('now overall: ', df['label'].value_counts())\n",
    "\n",
    "# Filter dataframe: only rows with > 2 words in review_description\n",
    "df = df[df['review_description'].apply(lambda x: len(x.split()) > 2)]\n",
    "\n",
    "# Convert emojis into text\n",
    "demoji.download_codes()\n",
    "def replace_emojis(text):\n",
    "    emojis = demoji.findall(text)\n",
    "    for emo, desc in emojis.items():\n",
    "        text = text.replace(emo, f\":{desc}:\")\n",
    "    return text\n",
    "\n",
    "df['review_description'] = df['review_description'].apply(replace_emojis)\n",
    "\n",
    "# Configure Generative AI API (update with your API key)\n",
    "genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def extract_text(response):\n",
    "    if not response.candidates:\n",
    "        return \"\"\n",
    "    candidate = response.candidates[0]\n",
    "    if not candidate.content.parts:\n",
    "        return \"\"\n",
    "    return \"\".join([part.text for part in candidate.content.parts])\n",
    "\n",
    "def translate_and_correct_grammar(text):\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-001')\n",
    "    # Translate from Hindi to English\n",
    "    translate_prompt = f\"Translate the following text to English, just return the translation:\\n\\n{text}\"\n",
    "    translated_response = model.generate_content(translate_prompt)\n",
    "    translated_text = extract_text(translated_response)\n",
    "    # Correct grammar\n",
    "    correct_prompt = f\"Rewrite the following sentence and fix any grammar issues. If none, just return the sentence:\\n\\n{translated_text}\"\n",
    "    corrected_response = model.generate_content(correct_prompt)\n",
    "    corrected_text = extract_text(corrected_response)\n",
    "    return corrected_text\n",
    "\n",
    "def process_review(row):\n",
    "    return translate_and_correct_grammar(row['review_description'])\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "# Process reviews in batches to avoid rate limiting\n",
    "for start_index in range(0, len(df1), 7):\n",
    "    time.sleep(50)\n",
    "    df_next_7 = df1.iloc[start_index:start_index+7].copy()\n",
    "    df_next_7['review_description'] = df_next_7.apply(process_review, axis=1)\n",
    "    df_new = pd.concat([df_new, df_next_7], ignore_index=True)\n",
    "\n",
    "df.update(df_new)\n",
    "df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Spelling correction\n",
    "sp = SpellCorrectionModel(language='en')\n",
    "sp.load('en_large.pkl/en_large.pkl')\n",
    "df['review_description'] = df['review_description'].apply(\n",
    "    lambda x: x.get('spell_corrected_text') if isinstance(x, dict) and 'spell_corrected_text' in x else x\n",
    ")\n",
    "\n",
    "# Lemmatization using NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['review_description'] = df['review_description'].apply(\n",
    "    lambda text: \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
    ")\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df['review_description_after_removing_stop_words'] = df['review_description'].apply(remove_stopwords)\n",
    "\n",
    "# Remove numbers\n",
    "pattern = r'[0-9]'\n",
    "df['review_description'] = df['review_description'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Remove emojis using emoji module\n",
    "df['review_description'] = df['review_description'].apply(lambda x: emoji.replace_emoji(x, ''))\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(lambda x: emoji.replace_emoji(x, ''))\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['review_description'] = df['review_description'].apply(lambda x: x.lower())\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['review_description'] = df['review_description'].apply(remove_punctuation)\n",
    "df['review_description_after_removing_stop_words'] = df['review_description_after_removing_stop_words'].apply(remove_punctuation)\n",
    "\n",
    "# Drop rows where review_description is NaN or empty\n",
    "df = df.dropna(subset=['review_description'])\n",
    "df = df[df['review_description'].str.strip() != '']\n",
    "df = df[df['review_description'].apply(lambda x: len(x.split()) > 2)]\n",
    "df = df[df['review_description_after_removing_stop_words'].apply(lambda x: len(x.split()) > 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Family_Hotel_love_&_match_3_US.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
